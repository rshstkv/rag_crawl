"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LlamaIndex.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã LlamaIndex –¥–ª—è RAG —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.
"""

import logging
import time
from typing import List, Dict, Any, Optional
from pathlib import Path
import uuid

from llama_index.core import VectorStoreIndex, Document, StorageContext, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.ingestion import IngestionPipeline
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.vector_stores.qdrant import QdrantVectorStore
import qdrant_client
from qdrant_client.models import Distance, VectorParams
from fastapi import UploadFile
from sqlalchemy.orm import Session

from ..config import settings
from ..database.models import Document as DBDocument, ChatHistory, DocumentChunk
from ..utils.text_processing import extract_text_from_file, clean_text, sanitize_filename

logger = logging.getLogger(__name__)


class LlamaIndexService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LlamaIndex.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.
    """

    def __init__(self, db: Session):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞."""
        self.db = db
        self._vector_store = None
        self._indices = {}  # –ö—ç—à –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ namespace
        self._chat_engines = {}  # –ö—ç—à chat engines –ø–æ namespace
        self._setup_llama_index()
        
    def _setup_llama_index(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ LlamaIndex."""
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM - –∏—Å–ø–æ–ª—å–∑—É–µ–º AzureOpenAI
        self.llm = AzureOpenAI(
            model=settings.azure_openai_chat_model,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ environment
            deployment_name=settings.azure_openai_chat_deployment,
            api_key=settings.azure_openai_api_key,
            azure_endpoint=settings.azure_openai_endpoint,
            api_version=settings.azure_openai_api_version,
            temperature=0.1,
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Embedding –º–æ–¥–µ–ª–∏
        self.embed_model = AzureOpenAIEmbedding(
            model=settings.azure_openai_embedding_model,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ environment
            deployment_name=settings.azure_openai_embedding_deployment,
            api_key=settings.azure_openai_api_key,
            azure_endpoint=settings.azure_openai_endpoint,
            api_version=settings.azure_openai_api_version,
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ Settings
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        Settings.node_parser = SentenceSplitter(
            chunk_size=settings.max_chunk_size,
            chunk_overlap=settings.chunk_overlap
        )
        
        logger.info("LlamaIndex –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å Azure OpenAI")
    
    def _detect_document_category(self, title: str, source_type: str, content: str) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É."""
        title_lower = title.lower()
        content_lower = content.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Ñ–∞–π–ª–∞
        if source_type in ['md', 'markdown']:
            return "documentation"
        elif source_type in ['pdf']:
            return "document"
        elif source_type in ['txt']:
            return "text"
        elif source_type in ['html', 'htm']:
            return "web_page"
        elif source_type in ['docx', 'doc']:
            return "office_document"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        if any(word in title_lower for word in ['readme', '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', 'guide']):
            return "documentation"
        elif any(word in title_lower for word in ['api', 'reference', 'spec']):
            return "reference"
        elif any(word in title_lower for word in ['tutorial', '—É—Ä–æ–∫', '–æ–±—É—á–µ–Ω–∏–µ']):
            return "tutorial"
        elif any(word in title_lower for word in ['–Ω–æ–≤–æ—Å—Ç–∏', 'news', '—Å—Ç–∞—Ç—å—è', 'article']):
            return "article"
        elif any(word in title_lower for word in ['–æ—Ç—á–µ—Ç', 'report', '–∞–Ω–∞–ª–∏–∑']):
            return "report"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        if len(content) < 500:
            return "snippet"
        elif '```' in content or 'def ' in content or 'function ' in content:
            return "code_documentation"
        elif content.count('\n#') > 3:  # –º–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            return "structured_document"
        
        return "general"

    def _get_vector_store(self) -> QdrantVectorStore:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ Qdrant –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ LlamaIndex)."""
        try:
            client = qdrant_client.QdrantClient(
                host=settings.qdrant_host,    # localhost
                port=settings.qdrant_port,    # 6333
                timeout=30,                   # –¢–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                prefer_grpc=False            # –ò—Å–ø–æ–ª—å–∑—É–µ–º HTTP –≤–º–µ—Å—Ç–æ gRPC
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            collection_name = settings.qdrant_collection_name
            try:
                client.get_collection(collection_name)
                logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            except Exception:
                # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –µ—ë
                client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=1536,  # –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ Azure OpenAI embeddings
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}'")
            
            vector_store = QdrantVectorStore(
                client=client,
                collection_name=collection_name
            )
            
            logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è QdrantVectorStore: {settings.qdrant_host}:{settings.qdrant_port}/{collection_name}")
            return vector_store
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {e}")
            raise

    def _get_vector_store_lazy(self) -> Any:
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è vector store."""
        if self._vector_store is None:
            self._vector_store = self._get_vector_store()
        return self._vector_store

    def _get_index(self, namespace: str) -> VectorStoreIndex:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è namespace (–æ–±–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è Qdrant)."""
        if namespace not in self._indices:
            vector_store = self._get_vector_store_lazy()
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –∏–∑ Qdrant
            try:
                self._indices[namespace] = VectorStoreIndex.from_vector_store(
                    vector_store=vector_store,
                    storage_context=storage_context
                )
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π Qdrant –∏–Ω–¥–µ–∫—Å –¥–ª—è namespace: {namespace}")
            except Exception as e:
                # –ï—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
                logger.info(f"–°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π Qdrant –∏–Ω–¥–µ–∫—Å –¥–ª—è namespace: {namespace} (–ø—Ä–∏—á–∏–Ω–∞: {e})")
                self._indices[namespace] = VectorStoreIndex(
                    [],
                    storage_context=storage_context
                )
                logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π Qdrant –∏–Ω–¥–µ–∫—Å –¥–ª—è namespace: {namespace}")
            
        return self._indices[namespace]

    def _get_chat_engine(self, namespace: str, session_id: Optional[str] = None):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ chat engine –¥–ª—è namespace —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏."""
        cache_key = f"{namespace}:{session_id or 'default'}"
        
        if cache_key not in self._chat_engines:
            index = self._get_index(namespace)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            node_postprocessors = [
                SimilarityPostprocessor(similarity_cutoff=0.75)  # –¢–æ—Ç –∂–µ —Ñ–∏–ª—å—Ç—Ä —á—Ç–æ –∏ –≤ query
            ]
            
            # –°–æ–∑–¥–∞–µ–º chat engine —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            chat_engine = index.as_chat_engine(
                chat_mode="context",
                similarity_top_k=10,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                node_postprocessors=node_postprocessors,
                verbose=True
            )
            
            self._chat_engines[cache_key] = chat_engine
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω chat engine –¥–ª—è namespace: {namespace}")
        
        return self._chat_engines[cache_key]

    def _get_collection_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant."""
        try:
            vector_store = self._get_vector_store_lazy()
            client = vector_store.client
            collection_name = settings.qdrant_collection_name
            
            collection_info = client.get_collection(collection_name)
            
            return {
                "collection_name": collection_name,
                "points_count": collection_info.points_count,
                "vectors_count": collection_info.vectors_count,
                "status": collection_info.status,
                "config": {
                    "distance": collection_info.config.params.vectors.distance,
                    "size": collection_info.config.params.vectors.size,
                }
            }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            return {"error": str(e)}

    def _get_collection_points_count(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ—á–µ–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        try:
            info = self._get_collection_info()
            return info.get("points_count", 0)
        except Exception:
            return 0

    async def upload_document(
        self, 
        file: UploadFile, 
        namespace: str = "default"
    ) -> DBDocument:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ IngestionPipeline (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏).
        """
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞
            text_content = await extract_text_from_file(file)
            cleaned_text = clean_text(text_content)
            
            if not cleaned_text.strip():
                raise ValueError("–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ LlamaIndex —Å –±–æ–≥–∞—Ç—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            title = sanitize_filename(file.filename or "unknown")
            source_type = Path(file.filename or "").suffix.lower()[1:] or "unknown"
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_category = self._detect_document_category(title, source_type, cleaned_text)
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –¥–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            points_count_before = self._get_collection_points_count()
            
            # –ë–æ–≥–∞—Ç—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            rich_metadata = {
                "title": title,
                "source_type": source_type,
                "category": document_category,
                "namespace": namespace,
                "filename": file.filename,
                "content_length": len(cleaned_text),
                "content_preview": cleaned_text[:200] + "..." if len(cleaned_text) > 200 else cleaned_text,
                "upload_source": "file_upload",
                "language": "ru",
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ –ë–î —Å–Ω–∞—á–∞–ª–∞, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å document_id
            db_document = DBDocument(
                title=title,
                source_type=source_type,
                namespace=namespace,
                source_url=file.filename,
                content_hash=str(hash(cleaned_text)),
                vector_id=str(uuid.uuid4()),
                chunks_count=0,  # –ü–æ–∫–∞ 0, –æ–±–Ω–æ–≤–∏–º –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–æ–≤
                metadata_json=rich_metadata
            )
            
            self.db.add(db_document)
            self.db.commit()
            self.db.refresh(db_document)
            
            # –î–æ–±–∞–≤–ª—è–µ–º document_id –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            rich_metadata["document_id"] = db_document.id
            
            llama_doc = Document(
                text=cleaned_text,
                metadata=rich_metadata
            )
            
            # –ü–æ–ª—É—á–∞–µ–º vector store –¥–ª—è pipeline
            vector_store = self._get_vector_store_lazy()
            
            logger.info(f"üìä –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {title}")
            logger.info(f"üîç Vector store: {type(vector_store).__name__}")
            
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–Ω–∏–µ pipeline –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞
            pipeline = IngestionPipeline(
                transformations=[
                    SentenceSplitter(
                        chunk_size=settings.max_chunk_size,    # 1024
                        chunk_overlap=settings.chunk_overlap   # 200
                    ),
                    self.embed_model,
                ],
                vector_store=vector_store,
            )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ pipeline (—ç—Ç–æ —Å–æ–∑–¥–∞—Å—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏)
            nodes = pipeline.run(documents=[llama_doc])
            chunk_count = len(nodes)
            
            logger.info(f"üìù –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {chunk_count}")
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ node
            for i, node in enumerate(nodes):
                chunk_metadata = {**rich_metadata, "chunk_index": i}
                chunk = DocumentChunk(
                    document_id=db_document.id,
                    chunk_index=i,
                    content=node.text,
                    vector_id=node.node_id,
                    metadata_json=chunk_metadata
                )
                self.db.add(chunk)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            db_document.chunks_count = chunk_count
            self.db.commit()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
            points_count_after = self._get_collection_points_count()
            vectors_added = points_count_after - points_count_before
            
            logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã: {vectors_added}")
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –¥–ª—è namespace
            if namespace in self._indices:
                del self._indices[namespace]
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ chat engines –¥–ª—è —ç—Ç–æ–≥–æ namespace
            keys_to_remove = [k for k in self._chat_engines.keys() if k.startswith(f"{namespace}:")]
            for key in keys_to_remove:
                del self._chat_engines[key]
            
            logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç '{title}' –∑–∞–≥—Ä—É–∂–µ–Ω –≤ namespace '{namespace}', —Å–æ–∑–¥–∞–Ω–æ {chunk_count} —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ IngestionPipeline")
            
            return db_document
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            self.db.rollback()
            raise

    async def reindex_all_documents(self, namespace: Optional[str] = None) -> Dict[str, Any]:
        """–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ namespace."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            query = self.db.query(DBDocument).filter(DBDocument.is_active == True)
            if namespace:
                query = query.filter(DBDocument.namespace == namespace)
            
            documents = query.all()
            
            if not documents:
                return {
                    "message": "–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏",
                    "namespace": namespace,
                    "documents_processed": 0
                }
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à–∏
            if namespace:
                if namespace in self._indices:
                    del self._indices[namespace]
                keys_to_remove = [k for k in self._chat_engines.keys() if k.startswith(f"{namespace}:")]
                for key in keys_to_remove:
                    del self._chat_engines[key]
            else:
                self._indices.clear()
                self._chat_engines.clear()
            
            # –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            success_count = 0
            errors = []
            
            for document in documents:
                try:
                    result = await self.reindex_document(document.id)
                    if result["success"]:
                        success_count += 1
                    else:
                        errors.append(f"–î–æ–∫—É–º–µ–Ω—Ç {document.id}: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
                except Exception as e:
                    errors.append(f"–î–æ–∫—É–º–µ–Ω—Ç {document.id}: {str(e)}")
            
            return {
                "message": f"–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
                "namespace": namespace,
                "documents_processed": success_count,
                "total_documents": len(documents),
                "errors": errors
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            raise

    async def reindex_document(self, document_id: int) -> Dict[str, Any]:
        """–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            document = self.db.query(DBDocument).filter(
                DBDocument.id == document_id,
                DBDocument.is_active == True
            ).first()
            
            if not document:
                return {
                    "success": False,
                    "error": "–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
                }
            
            # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunks = self.db.query(DocumentChunk).filter(
                DocumentChunk.document_id == document_id
            ).all()
            
            if not chunks:
                return {
                    "success": False,
                    "error": "–ß–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
                }
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤
            full_text = "\n".join([chunk.content for chunk in chunks])
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "title": document.title,
                "source_type": document.source_type,
                "category": document.metadata_json.get("category", "general") if document.metadata_json else "general",
                "namespace": document.namespace,
                "filename": document.source_url,
                "content_length": len(full_text),
                "upload_source": "reindex",
                "language": "ru",
                "document_id": document.id
            }
            
            # –°–æ–∑–¥–∞–µ–º LlamaIndex –¥–æ–∫—É–º–µ–Ω—Ç
            llama_doc = Document(
                text=full_text,
                metadata=metadata
            )
            
            # –ü–æ–ª—É—á–∞–µ–º vector store
            vector_store = self._get_vector_store_lazy()
            
            # –°–æ–∑–¥–∞–µ–º pipeline –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            pipeline = IngestionPipeline(
                transformations=[
                    SentenceSplitter(
                        chunk_size=settings.max_chunk_size,
                        chunk_overlap=settings.chunk_overlap
                    ),
                    self.embed_model,
                ],
                vector_store=vector_store,
            )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            nodes = pipeline.run(documents=[llama_doc])
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            self.db.query(DocumentChunk).filter(
                DocumentChunk.document_id == document_id
            ).delete()
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —á–∞–Ω–∫–∏
            for i, node in enumerate(nodes):
                chunk_metadata = {**metadata, "chunk_index": i}
                chunk = DocumentChunk(
                    document_id=document.id,
                    chunk_index=i,
                    content=node.text,
                    vector_id=node.node_id,
                    metadata_json=chunk_metadata
                )
                self.db.add(chunk)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
            document.chunks_count = len(nodes)
            self.db.commit()
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à–∏ –¥–ª—è namespace
            namespace = document.namespace
            if namespace in self._indices:
                del self._indices[namespace]
            
            keys_to_remove = [k for k in self._chat_engines.keys() if k.startswith(f"{namespace}:")]
            for key in keys_to_remove:
                del self._chat_engines[key]
            
            logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {document_id} –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω, —Å–æ–∑–¥–∞–Ω–æ {len(nodes)} —á–∞–Ω–∫–æ–≤")
            
            return {
                "success": True,
                "document_id": document_id,
                "chunks_created": len(nodes),
                "message": "–î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω"
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            self.db.rollback()
            return {
                "success": False,
                "error": str(e)
            }

    async def reindex_documents_batch(self, document_ids: List[int]) -> Dict[str, Any]:
        """–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≥—Ä—É–ø–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        try:
            results = []
            success_count = 0
            
            for doc_id in document_ids:
                result = await self.reindex_document(doc_id)
                results.append(result)
                if result["success"]:
                    success_count += 1
            
            return {
                "message": f"–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
                "documents_processed": success_count,
                "total_documents": len(document_ids),
                "results": results
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
            raise

    async def get_document_content(self, document_id: int) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —á–∞–Ω–∫–∞–º–∏."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            document = self.db.query(DBDocument).filter(
                DBDocument.id == document_id,
                DBDocument.is_active == True
            ).first()
            
            if not document:
                return {"error": "–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"}
            
            # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏
            chunks = self.db.query(DocumentChunk).filter(
                DocumentChunk.document_id == document_id
            ).order_by(DocumentChunk.chunk_index).all()
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
            full_text = "\n".join([chunk.content for chunk in chunks])
            
            return {
                "document": {
                    "id": document.id,
                    "title": document.title,
                    "source_type": document.source_type,
                    "namespace": document.namespace,
                    "created_at": document.created_at.isoformat(),
                    "chunks_count": document.chunks_count,
                    "metadata": document.metadata_json or {}
                },
                "full_text": full_text,
                "chunks": [
                    {
                        "index": chunk.chunk_index,
                        "content": chunk.content,
                        "vector_id": chunk.vector_id,
                        "metadata": chunk.metadata_json or {}
                    }
                    for chunk in chunks
                ]
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return {"error": str(e)}

    async def get_document_chunks(self, document_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""
        try:
            chunks = self.db.query(DocumentChunk).filter(
                DocumentChunk.document_id == document_id
            ).order_by(DocumentChunk.chunk_index).all()
            
            return [
                {
                    "id": chunk.id,
                    "chunk_index": chunk.chunk_index,
                    "content": chunk.content,
                    "vector_id": chunk.vector_id,
                    "metadata": chunk.metadata_json or {},
                    "created_at": chunk.created_at.isoformat()
                }
                for chunk in chunks
            ]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return []

    async def get_system_diagnostics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ."""
        try:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ Qdrant
            qdrant_info = self._get_collection_info()
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –≤ PostgreSQL
            total_documents = self.db.query(DBDocument).filter(DBDocument.is_active == True).count()
            total_chunks = self.db.query(DocumentChunk).count()
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ namespace
            from sqlalchemy import func
            namespace_stats = self.db.query(
                DBDocument.namespace,
                func.count(DBDocument.id).label("document_count")
            ).filter(DBDocument.is_active == True).group_by(DBDocument.namespace).all()
            
            return {
                "qdrant": qdrant_info,
                "postgresql": {
                    "total_documents": total_documents,
                    "total_chunks": total_chunks,
                    "namespace_stats": [
                        {"namespace": ns, "documents": count}
                        for ns, count in namespace_stats
                    ]
                },
                "cache": {
                    "indices_cached": len(self._indices),
                    "chat_engines_cached": len(self._chat_engines)
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: {e}")
            return {"error": str(e)}

    async def chat(
        self, 
        message: str, 
        namespace: str = "default",
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        –ß–∞—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π ChatEngine.
        """
        try:
            chat_engine = self._get_chat_engine(namespace, session_id)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π chat engine
            response = chat_engine.chat(message)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources = []
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for node in response.source_nodes:
                    if hasattr(node, 'metadata'):
                        sources.append({
                            "document_title": node.metadata.get("title", "Unknown"),
                            "source_type": node.metadata.get("source_type", "unknown"),
                            "score": getattr(node, 'score', 0.0)
                        })
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–æ–≤
            chat_record = ChatHistory(
                session_id=session_id or "default",
                namespace=namespace,
                user_message=message,
                assistant_message=str(response),
                sources_used=sources
            )
            
            self.db.add(chat_record)
            self.db.commit()
            
            return {
                "response": str(response),
                "sources": sources,
                "session_id": session_id
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
            raise

    async def query(
        self, 
        question: str, 
        namespace: str = "default"
    ) -> Dict[str, Any]:
        """
        –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ debug –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.
        """
        try:
            start_time = time.time()
            
            # –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_info = self._get_collection_info()
            
            index = self._get_index(namespace)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            node_postprocessors = [
                SimilarityPostprocessor(similarity_cutoff=0.75)  # –§–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            ]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ query engine —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
            query_engine = index.as_query_engine(
                similarity_top_k=10,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                node_postprocessors=node_postprocessors,
                verbose=True
            )
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            response = query_engine.query(question)
            
            search_time = (time.time() - start_time) * 1000
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            sources = []
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for node in response.source_nodes:
                    if hasattr(node, 'metadata'):
                        sources.append({
                            "document_title": node.metadata.get("title", "Unknown"),
                            "source_type": node.metadata.get("source_type", "unknown"),
                            "score": round(getattr(node, 'score', 0.0), 3),  # –û–∫—Ä—É–≥–ª—è–µ–º –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
                            "document_id": node.metadata.get("document_id"),
                            "chunk_index": node.metadata.get("chunk_index", 0),
                            "content_preview": node.text[:200] + "..." if len(node.text) > 200 else node.text
                        })
            
            logger.info(f"‚úÖ Query –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(sources)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (>0.75)")
            
            return {
                "response": str(response),
                "sources": sources,
                "debug_info": {
                    "collection_points_count": collection_info.get("points_count", 0),
                    "similarity_cutoff": 0.75,
                    "search_time_ms": round(search_time, 2),
                    "namespace": namespace,
                    "sources_found": len(sources)
                },
                "total_documents": collection_info.get("points_count", 0),
                "search_time_ms": round(search_time, 2)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            raise

    async def get_documents(
        self, 
        namespace: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        try:
            query = self.db.query(DBDocument).filter(DBDocument.is_active == True)
            
            if namespace:
                query = query.filter(DBDocument.namespace == namespace)
            
            documents = query.order_by(DBDocument.created_at.desc()).all()
            
            return [
                {
                    "id": doc.id,
                    "title": doc.title,
                    "source_type": doc.source_type,
                    "namespace": doc.namespace,
                    "created_at": doc.created_at.isoformat(),
                    "chunks_count": doc.chunks_count,
                    "metadata": doc.metadata_json or {},
                    "size": len(str(doc.metadata_json or {})),
                }
                for doc in documents
            ]
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ, –∞ –Ω–µ –ø–æ–¥–Ω–∏–º–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
            return []

    async def delete_document(self, document_id: int) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        try:
            document = self.db.query(DBDocument).filter(
                DBDocument.id == document_id,
                DBDocument.is_active == True
            ).first()
            
            if not document:
                return False
            
            # –ú—è–≥–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ
            document.is_active = False
            self.db.commit()
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π
            namespace = document.namespace
            if namespace in self._indices:
                del self._indices[namespace]
            
            keys_to_remove = [k for k in self._chat_engines.keys() if k.startswith(f"{namespace}:")]
            for key in keys_to_remove:
                del self._chat_engines[key]
            
            logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç {document_id} —É–¥–∞–ª–µ–Ω")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            self.db.rollback()
            raise

    async def get_chat_history(
        self,
        session_id: Optional[str] = None,
        namespace: Optional[str] = None,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–æ–≤."""
        try:
            query = self.db.query(ChatHistory)
            
            if session_id:
                query = query.filter(ChatHistory.session_id == session_id)
            
            if namespace:
                query = query.filter(ChatHistory.namespace == namespace)
            
            history = query.order_by(
                ChatHistory.created_at.desc()
            ).limit(limit).all()
            
            return [
                {
                    "id": record.id,
                    "session_id": record.session_id,
                    "namespace": record.namespace,
                    "user_message": record.user_message,
                    "assistant_message": record.assistant_message,
                    "created_at": record.created_at.isoformat()
                }
                for record in history
            ]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            raise

    async def resync_namespace(self, namespace: str) -> Dict[str, Any]:
        """–†–µ—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ namespace."""
        try:
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π
            if namespace in self._indices:
                del self._indices[namespace]
            
            keys_to_remove = [k for k in self._chat_engines.keys() if k.startswith(f"{namespace}:")]
            for key in keys_to_remove:
                del self._chat_engines[key]
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            doc_count = self.db.query(DBDocument).filter(
                DBDocument.namespace == namespace,
                DBDocument.is_active == True
            ).count()
            
            logger.info(f"Namespace '{namespace}' —Ä–µ—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            return {
                "namespace": namespace,
                "documents_count": doc_count,
                "message": "–†–µ—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞"
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–µ—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: {e}")
            raise

    def index_document(self, document: 'DBDocument') -> bool:
        """
        –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –¢–µ–ø–µ—Ä—å –ø–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ chunks.
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunks = self.db.query(DocumentChunk).filter(
                DocumentChunk.document_id == document.id
            ).order_by(DocumentChunk.chunk_index).all()
            
            if not chunks:
                logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document.id}")
                return False
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤
            full_text = "\n".join([chunk.content for chunk in chunks])
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç LlamaIndex
            llama_doc = Document(
                text=full_text,
                metadata={
                    "title": document.title,
                    "source_type": document.source_type,
                    "namespace": document.namespace,
                    "document_id": document.id,
                    "vector_id": document.vector_id,
                }
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è namespace
            index = self._get_index(document.namespace)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∏–Ω–¥–µ–∫—Å
            index.insert(llama_doc)
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à –¥–ª—è namespace
            if document.namespace in self._chat_engines:
                del self._chat_engines[document.namespace]
            
            logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç {document.id} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –≤ namespace {document.namespace}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document.id}: {e}")
            raise 

    def chat_with_docs(self, message: str, namespace: str = "default") -> str:
        """
        –ß–∞—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π ChatEngine.
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º chat engine –¥–ª—è namespace
            chat_engine = self._get_chat_engine(namespace)
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π chat engine
            response = chat_engine.chat(message)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources = []
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for node in response.source_nodes:
                    if hasattr(node, 'metadata'):
                        sources.append({
                            "document_title": node.metadata.get("title", "Unknown"),
                            "source_type": node.metadata.get("source_type", "unknown"),
                            "score": getattr(node, 'score', 0.0)
                        })
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
            result = str(response)
            
            if sources:
                result += "\n\nüìö **–ò—Å—Ç–æ—á–Ω–∏–∫–∏:**\n"
                for i, source in enumerate(sources, 1):
                    title = source.get('document_title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç')
                    score = source.get('score', 0.0)
                    result += f"{i}. {title} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f})\n"
            
            logger.info(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –≤ namespace {namespace}")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞: {str(e)}" 